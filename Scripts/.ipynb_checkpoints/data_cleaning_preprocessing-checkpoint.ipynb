{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sent2vec.vectorizer import Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pandas to display the whole dataframe\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the cyberbullying raw data\n",
    "df_cyberbullying = pd.read_csv('../Raw Data/cyberbullying_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the toxic tweets raw data\n",
    "df_abuse = pd.read_csv('../Raw Data/Toxic_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the labels of each datapoint to match each other\n",
    "df_cyberbullying['label'] = df_cyberbullying['label'].replace(-1, 'toxic')\n",
    "df_cyberbullying['label'] = df_cyberbullying['label'].replace(0, 'not_toxic')\n",
    "\n",
    "df_abuse['Toxicity'] = df_abuse['Toxicity'].replace(0, 'not_toxic')\n",
    "df_abuse['Toxicity'] = df_abuse['Toxicity'].replace(1, 'toxic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra column in abuse_df\n",
    "df_abuse.drop(columns=['Unnamed: 0'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the columns of df_abuse\n",
    "df_abuse = df_abuse[['tweet', 'Toxicity']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns of both dataframes\n",
    "df_abuse = df_abuse.rename(columns={'tweet': 'comment', 'Toxicity': 'label'})\n",
    "df_cyberbullying = df_cyberbullying.rename(columns={'headline': 'comment', 'label': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove some non-English entries in the dataset\n",
    "df_cyberbullying = df_cyberbullying.drop(df_cyberbullying.index[15307:18148])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle both dataframes\n",
    "df_abuse = df_abuse.sample(frac=1).reset_index(drop=True)\n",
    "df_cyberbullying = df_cyberbullying.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the first 15000 entries of df_abuse (dataset is too large otherwise)\n",
    "df_abuse = df_abuse.head(15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the 2 dataframes into 1\n",
    "df = pd.concat([df_abuse, df_cyberbullying])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  \n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  import sys\n",
      "/Users/nikhildixit/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "# Remove all twitter handles and hashtags from the dataset\n",
    "df['comment'] = df['comment'].str.replace(r'@([A-Za-z0-9_]+)', '')\n",
    "df['comment'] = df['comment'].str.replace(r'#([A-Za-z0-9_]+)', '')\n",
    "\n",
    "# Remove all punctuation from dataset\n",
    "df['comment'] = df['comment'].str.replace(r'[^\\w\\s]+', '')\n",
    "df['comment'] = df['comment'].str.replace(r'\\d+', '')\n",
    "\n",
    "# Lowercase all comments\n",
    "df['comment'] = df['comment'].str.lower()\n",
    "\n",
    "# Remove all non-ASCII characters in the dataset\n",
    "df['comment'] = df['comment'].str.replace(r'[^\\x00-\\x7F]+', '')\n",
    "\n",
    "# Trim excess whitespace around each entry\n",
    "df['comment'] = df['comment'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/nikhildixit/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize the corpus and remove stopwords\n",
    "nltk.download('wordnet');\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized = []\n",
    "    for w in text.split(' '):\n",
    "        if w not in stop_words and len(w) > 0:\n",
    "            lemmatized.append(lemmatizer.lemmatize(w))\n",
    "    \n",
    "    cleaned_text = ' '.join(lemmatized)\n",
    "    return cleaned_text\n",
    "\n",
    "df['comment'] = df['comment'].apply(lemmatize_text)\n",
    "df['comment'] = df['comment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all blank comments with NaN\n",
    "df = df.replace(r'^s*$', float('NaN'), regex = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all NaN values\n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the dataframe's index column\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the extra index column in the dataframe\n",
    "df.drop(columns=['index'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>buddy fruit brandon said dirty as coon</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>follow lineup announcement today</td>\n",
       "      <td>not_toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>like said vacuous drivel patent nonsense skill talking as certainly mastered izak</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>huh threaten as hole content blank remove added content</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fuck u fuck vandal</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30160</th>\n",
       "      <td>notice polka dick course beautiful send pat comic deem worthy love reading</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30161</th>\n",
       "      <td>hello go fuck kid adult talking</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30162</th>\n",
       "      <td>rt video derek jeter hit walkoff single final atbat yankee stadium httptcojmbiagxu httptcocij</td>\n",
       "      <td>not_toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30163</th>\n",
       "      <td>know talking two primary source ambrosius gildas main source pseudo nennius make use gildas plus add snippet chronicle material folklore cited previous version article along secondary source bede geoffrey reliable modern scholarly source chadwick woolf gidlow fleuriot respectable scholar need leave article alone unclear source</td>\n",
       "      <td>not_toxic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30164</th>\n",
       "      <td>everyone info give flying fuck account get deleted block changed ip use different email become troll vandel jack grow</td>\n",
       "      <td>toxic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30165 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                        comment  \\\n",
       "0                                                                                                                                                                                                                                                                                                        buddy fruit brandon said dirty as coon   \n",
       "1                                                                                                                                                                                                                                                                                                              follow lineup announcement today   \n",
       "2                                                                                                                                                                                                                                                             like said vacuous drivel patent nonsense skill talking as certainly mastered izak   \n",
       "3                                                                                                                                                                                                                                                                                       huh threaten as hole content blank remove added content   \n",
       "4                                                                                                                                                                                                                                                                                                                            fuck u fuck vandal   \n",
       "...                                                                                                                                                                                                                                                                                                                                         ...   \n",
       "30160                                                                                                                                                                                                                                                                notice polka dick course beautiful send pat comic deem worthy love reading   \n",
       "30161                                                                                                                                                                                                                                                                                                           hello go fuck kid adult talking   \n",
       "30162                                                                                                                                                                                                                                             rt video derek jeter hit walkoff single final atbat yankee stadium httptcojmbiagxu httptcocij   \n",
       "30163  know talking two primary source ambrosius gildas main source pseudo nennius make use gildas plus add snippet chronicle material folklore cited previous version article along secondary source bede geoffrey reliable modern scholarly source chadwick woolf gidlow fleuriot respectable scholar need leave article alone unclear source   \n",
       "30164                                                                                                                                                                                                                     everyone info give flying fuck account get deleted block changed ip use different email become troll vandel jack grow   \n",
       "\n",
       "           label  \n",
       "0          toxic  \n",
       "1      not_toxic  \n",
       "2          toxic  \n",
       "3          toxic  \n",
       "4          toxic  \n",
       "...          ...  \n",
       "30160      toxic  \n",
       "30161      toxic  \n",
       "30162  not_toxic  \n",
       "30163  not_toxic  \n",
       "30164      toxic  \n",
       "\n",
       "[30165 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = df['comment'].tolist() # Turn the comment column into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2') # Create the embedding model\n",
    "input_embeddings = model.encode(sentences) # Embed the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>374</th>\n",
       "      <th>375</th>\n",
       "      <th>376</th>\n",
       "      <th>377</th>\n",
       "      <th>378</th>\n",
       "      <th>379</th>\n",
       "      <th>380</th>\n",
       "      <th>381</th>\n",
       "      <th>382</th>\n",
       "      <th>383</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.061659</td>\n",
       "      <td>0.042924</td>\n",
       "      <td>-0.027909</td>\n",
       "      <td>-0.094398</td>\n",
       "      <td>-0.005569</td>\n",
       "      <td>-0.060932</td>\n",
       "      <td>0.148430</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>-0.032557</td>\n",
       "      <td>-0.025835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.056826</td>\n",
       "      <td>-0.018621</td>\n",
       "      <td>-0.008933</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.044936</td>\n",
       "      <td>0.043737</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>-0.047291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.078584</td>\n",
       "      <td>-0.017030</td>\n",
       "      <td>0.012841</td>\n",
       "      <td>0.042564</td>\n",
       "      <td>0.011090</td>\n",
       "      <td>0.093269</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>-0.015763</td>\n",
       "      <td>0.011048</td>\n",
       "      <td>0.052986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050060</td>\n",
       "      <td>0.085719</td>\n",
       "      <td>-0.048181</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.033048</td>\n",
       "      <td>-0.052803</td>\n",
       "      <td>0.060913</td>\n",
       "      <td>-0.099276</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.015926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018492</td>\n",
       "      <td>0.012215</td>\n",
       "      <td>0.039271</td>\n",
       "      <td>-0.066744</td>\n",
       "      <td>-0.140989</td>\n",
       "      <td>-0.005288</td>\n",
       "      <td>0.044681</td>\n",
       "      <td>0.044251</td>\n",
       "      <td>0.012833</td>\n",
       "      <td>-0.001225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091907</td>\n",
       "      <td>0.055456</td>\n",
       "      <td>-0.060903</td>\n",
       "      <td>-0.009517</td>\n",
       "      <td>-0.064932</td>\n",
       "      <td>-0.026313</td>\n",
       "      <td>0.098399</td>\n",
       "      <td>0.038947</td>\n",
       "      <td>0.070080</td>\n",
       "      <td>-0.012449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.028316</td>\n",
       "      <td>0.073130</td>\n",
       "      <td>-0.026052</td>\n",
       "      <td>0.070508</td>\n",
       "      <td>-0.013783</td>\n",
       "      <td>-0.002817</td>\n",
       "      <td>-0.028664</td>\n",
       "      <td>-0.081183</td>\n",
       "      <td>0.103029</td>\n",
       "      <td>-0.003240</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021038</td>\n",
       "      <td>-0.029412</td>\n",
       "      <td>0.114542</td>\n",
       "      <td>0.026422</td>\n",
       "      <td>0.009733</td>\n",
       "      <td>0.062683</td>\n",
       "      <td>0.031651</td>\n",
       "      <td>-0.008130</td>\n",
       "      <td>0.117193</td>\n",
       "      <td>-0.024545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.075620</td>\n",
       "      <td>0.070810</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>-0.102053</td>\n",
       "      <td>0.062714</td>\n",
       "      <td>-0.024888</td>\n",
       "      <td>0.065374</td>\n",
       "      <td>-0.006156</td>\n",
       "      <td>0.008878</td>\n",
       "      <td>-0.009752</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030477</td>\n",
       "      <td>-0.046901</td>\n",
       "      <td>-0.055749</td>\n",
       "      <td>-0.011700</td>\n",
       "      <td>-0.008204</td>\n",
       "      <td>0.015479</td>\n",
       "      <td>-0.013746</td>\n",
       "      <td>0.034643</td>\n",
       "      <td>0.087525</td>\n",
       "      <td>-0.085699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30160</th>\n",
       "      <td>0.037771</td>\n",
       "      <td>-0.038717</td>\n",
       "      <td>-0.012030</td>\n",
       "      <td>-0.023955</td>\n",
       "      <td>-0.041778</td>\n",
       "      <td>0.007615</td>\n",
       "      <td>0.030302</td>\n",
       "      <td>-0.056470</td>\n",
       "      <td>-0.003735</td>\n",
       "      <td>-0.004116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.018473</td>\n",
       "      <td>-0.088215</td>\n",
       "      <td>-0.032296</td>\n",
       "      <td>0.050388</td>\n",
       "      <td>-0.036271</td>\n",
       "      <td>0.108113</td>\n",
       "      <td>0.024633</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.125703</td>\n",
       "      <td>-0.134695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30161</th>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.035123</td>\n",
       "      <td>0.020931</td>\n",
       "      <td>-0.025798</td>\n",
       "      <td>-0.079267</td>\n",
       "      <td>0.068680</td>\n",
       "      <td>-0.035519</td>\n",
       "      <td>0.053660</td>\n",
       "      <td>-0.033886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062581</td>\n",
       "      <td>0.022047</td>\n",
       "      <td>-0.020913</td>\n",
       "      <td>0.010086</td>\n",
       "      <td>-0.067338</td>\n",
       "      <td>0.001637</td>\n",
       "      <td>0.018102</td>\n",
       "      <td>0.053420</td>\n",
       "      <td>0.033898</td>\n",
       "      <td>-0.032134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30162</th>\n",
       "      <td>-0.062721</td>\n",
       "      <td>-0.001441</td>\n",
       "      <td>0.010116</td>\n",
       "      <td>-0.027680</td>\n",
       "      <td>0.058150</td>\n",
       "      <td>0.072341</td>\n",
       "      <td>-0.006970</td>\n",
       "      <td>0.090408</td>\n",
       "      <td>0.076522</td>\n",
       "      <td>-0.018395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.025699</td>\n",
       "      <td>0.005768</td>\n",
       "      <td>-0.036862</td>\n",
       "      <td>0.026968</td>\n",
       "      <td>-0.112689</td>\n",
       "      <td>-0.041223</td>\n",
       "      <td>0.025079</td>\n",
       "      <td>0.049279</td>\n",
       "      <td>-0.032683</td>\n",
       "      <td>0.024724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30163</th>\n",
       "      <td>-0.003122</td>\n",
       "      <td>0.003426</td>\n",
       "      <td>-0.031749</td>\n",
       "      <td>0.018705</td>\n",
       "      <td>-0.028545</td>\n",
       "      <td>-0.054575</td>\n",
       "      <td>0.010559</td>\n",
       "      <td>-0.014771</td>\n",
       "      <td>0.020306</td>\n",
       "      <td>0.069420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018666</td>\n",
       "      <td>-0.074635</td>\n",
       "      <td>0.024131</td>\n",
       "      <td>0.054823</td>\n",
       "      <td>0.033122</td>\n",
       "      <td>-0.046527</td>\n",
       "      <td>0.087907</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.017821</td>\n",
       "      <td>0.073322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30164</th>\n",
       "      <td>-0.045666</td>\n",
       "      <td>-0.019678</td>\n",
       "      <td>0.014234</td>\n",
       "      <td>-0.069929</td>\n",
       "      <td>0.018715</td>\n",
       "      <td>0.020477</td>\n",
       "      <td>0.086048</td>\n",
       "      <td>-0.096252</td>\n",
       "      <td>0.066123</td>\n",
       "      <td>0.021096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064653</td>\n",
       "      <td>-0.024641</td>\n",
       "      <td>-0.034459</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>-0.023367</td>\n",
       "      <td>-0.010531</td>\n",
       "      <td>0.029118</td>\n",
       "      <td>-0.028840</td>\n",
       "      <td>-0.015747</td>\n",
       "      <td>-0.032373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>30165 rows × 384 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4         5         6    \\\n",
       "0     -0.061659  0.042924 -0.027909 -0.094398 -0.005569 -0.060932  0.148430   \n",
       "1     -0.078584 -0.017030  0.012841  0.042564  0.011090  0.093269  0.005907   \n",
       "2     -0.018492  0.012215  0.039271 -0.066744 -0.140989 -0.005288  0.044681   \n",
       "3     -0.028316  0.073130 -0.026052  0.070508 -0.013783 -0.002817 -0.028664   \n",
       "4      0.075620  0.070810  0.005195 -0.102053  0.062714 -0.024888  0.065374   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "30160  0.037771 -0.038717 -0.012030 -0.023955 -0.041778  0.007615  0.030302   \n",
       "30161  0.002563  0.072721  0.035123  0.020931 -0.025798 -0.079267  0.068680   \n",
       "30162 -0.062721 -0.001441  0.010116 -0.027680  0.058150  0.072341 -0.006970   \n",
       "30163 -0.003122  0.003426 -0.031749  0.018705 -0.028545 -0.054575  0.010559   \n",
       "30164 -0.045666 -0.019678  0.014234 -0.069929  0.018715  0.020477  0.086048   \n",
       "\n",
       "            7         8         9    ...       374       375       376  \\\n",
       "0      0.000449 -0.032557 -0.025835  ...  0.056826 -0.018621 -0.008933   \n",
       "1     -0.015763  0.011048  0.052986  ...  0.050060  0.085719 -0.048181   \n",
       "2      0.044251  0.012833 -0.001225  ...  0.091907  0.055456 -0.060903   \n",
       "3     -0.081183  0.103029 -0.003240  ...  0.021038 -0.029412  0.114542   \n",
       "4     -0.006156  0.008878 -0.009752  ...  0.030477 -0.046901 -0.055749   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "30160 -0.056470 -0.003735 -0.004116  ...  0.018473 -0.088215 -0.032296   \n",
       "30161 -0.035519  0.053660 -0.033886  ...  0.062581  0.022047 -0.020913   \n",
       "30162  0.090408  0.076522 -0.018395  ...  0.025699  0.005768 -0.036862   \n",
       "30163 -0.014771  0.020306  0.069420  ... -0.018666 -0.074635  0.024131   \n",
       "30164 -0.096252  0.066123  0.021096  ...  0.064653 -0.024641 -0.034459   \n",
       "\n",
       "            377       378       379       380       381       382       383  \n",
       "0      0.019845  0.003539  0.004085  0.044936  0.043737  0.035842 -0.047291  \n",
       "1      0.000932  0.033048 -0.052803  0.060913 -0.099276  0.015163  0.015926  \n",
       "2     -0.009517 -0.064932 -0.026313  0.098399  0.038947  0.070080 -0.012449  \n",
       "3      0.026422  0.009733  0.062683  0.031651 -0.008130  0.117193 -0.024545  \n",
       "4     -0.011700 -0.008204  0.015479 -0.013746  0.034643  0.087525 -0.085699  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "30160  0.050388 -0.036271  0.108113  0.024633  0.000704  0.125703 -0.134695  \n",
       "30161  0.010086 -0.067338  0.001637  0.018102  0.053420  0.033898 -0.032134  \n",
       "30162  0.026968 -0.112689 -0.041223  0.025079  0.049279 -0.032683  0.024724  \n",
       "30163  0.054823  0.033122 -0.046527  0.087907  0.004902  0.017821  0.073322  \n",
       "30164  0.004643 -0.023367 -0.010531  0.029118 -0.028840 -0.015747 -0.032373  \n",
       "\n",
       "[30165 rows x 384 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_embeddings = pd.DataFrame(input_embeddings)\n",
    "df_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Clean Data/clean_data.csv')\n",
    "df_embeddings.to_csv('../Clean Data/embeddings.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
